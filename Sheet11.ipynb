{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjJ6bnO8HaA5",
        "outputId": "709c469f-a805-4173-dd12-2fb7bfbb882c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.7.3)\n",
            "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (12.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.43.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: sentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install sentencePiece\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tx\n",
        "import numpy as np\n",
        "import io\n",
        "import tqdm\n",
        "import datetime \n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1aD6Wug6IAkO"
      },
      "outputs": [],
      "source": [
        "path = tf.keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Otc2kWBa8aR0"
      },
      "outputs": [],
      "source": [
        "m = 128\n",
        "vocab_size = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N89eKHiUtMqD"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = 'https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "filepath = \"nietzsche.txt\"\n",
        "\n",
        "with open(filepath, \"wb\") as file:\n",
        "    file.write(r.content)\n",
        "\n",
        "with open('nietzsche.txt','r') as file:\n",
        "    sample_text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DyHQyCF8KXAg"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as sp\n",
        "\n",
        "sp.SentencePieceTrainer.train(\n",
        "    input='nietzsche.txt', model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pOMOvSZBt-6g"
      },
      "outputs": [],
      "source": [
        "# deserialize the trained model file to load it in the correct format\n",
        "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
        "\n",
        "# load the model as a tokenizer that can be used inside a tensorflow model\n",
        "tokenizer = tx.SentencepieceTokenizer(\n",
        "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
        "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z96LaiPMuJUM",
        "outputId": "83221b99-ff62-4449-f462-106f96c21042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([184 254  96 ... 766 787  12], shape=(171498,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_jQEzxKsvRXT"
      },
      "outputs": [],
      "source": [
        "#tokenizer.detokenize(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jMWlA78JqB8",
        "outputId": "513b3c98-52ce-44fa-ef3f-0aef15fef852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([184 254  96 ... 766 787  12], shape=(171498,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "#print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TqDkzVNKsH2",
        "outputId": "ef13848e-9ce8-47a4-b0ee-c7815a55881c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (129,), types: tf.int32>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "width_tokens = tx.sliding_window(tokens, m+1)\n",
        "\n",
        "text_dataset = tf.data.Dataset.from_tensor_slices(width_tokens)\n",
        "text_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "poZa6S9TLWFT"
      },
      "outputs": [],
      "source": [
        "def prepare(ds):   \n",
        "  ds = ds.map(lambda data: (data[:m], data[-1]))\n",
        "  \n",
        "  # cache \n",
        "  ds = ds.cache()\n",
        "  # shuffle, batch, prefetch our dataset\n",
        "  ds = ds.shuffle(5000)\n",
        "  ds = ds.batch(256)\n",
        "  ds = ds.prefetch(1024)\n",
        "\n",
        "  # split dataset into data and target   \n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBGIV50fv0Ti",
        "outputId": "e47f3ebb-150c-446a-dba4-077cc021a6b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 128), (None,)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "text_dataset = prepare(text_dataset)\n",
        "text_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S5Nw2Fz4ynCb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Reshape, Dropout, LayerNormalization, LeakyReLU, Embedding, MultiHeadAttention, GlobalAveragePooling1D, Add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uwo9H38d3I0J"
      },
      "outputs": [],
      "source": [
        "class Encoding_layer(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Encoding_layer, self).__init__()\n",
        "\n",
        "    self.embedding = Embedding(input_dim=vocab_size, output_dim=m)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    indices = tf.range(0, m)\n",
        "\n",
        "    a = self.embedding(x)\n",
        "    b = self.embedding(indices)\n",
        "    c = a + b\n",
        "    return c\n",
        "\n",
        "class TransformerBlock(Model):\n",
        "  def __init__(self):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "                       \n",
        "    self.Attention = MultiHeadAttention(2, m)\n",
        "    self.Drop1 = Dropout(0.1)\n",
        "    self.Drop2 = Dropout(0.1)\n",
        "\n",
        "    self.Dense1 = Dense(128, activation='relu')\n",
        "    self.Dense2 = Dense(m)\n",
        "    \n",
        "    self.LayNorm1 = LayerNormalization(epsilon=1e-6)\n",
        "    self.LayNorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    \n",
        "    in_out = self.Attention(x,x)\n",
        "    in_out = self.Drop1(in_out)\n",
        "    in_out = self.LayNorm1(in_out)\n",
        "    in_out = Add()([in_out, x])\n",
        "    in_out = self.LayNorm2(self.Drop2(self.Dense2(self.Dense1(in_out))))\n",
        "    in_out = Add()([in_out, x])\n",
        "\n",
        "    return in_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oBDLPfV83xWm"
      },
      "outputs": [],
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    self.loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)   \n",
        "    \n",
        "    self.metrics_list = [\n",
        "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                    tf.keras.metrics.CategoricalAccuracy(name=\"acc\")\n",
        "                    #tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
        "                   ]\n",
        "\n",
        "    self.layer_list = [\n",
        "      Encoding_layer(),\n",
        "      TransformerBlock(),\n",
        "      GlobalAveragePooling1D(),\n",
        "      Dense(vocab_size)\n",
        "    ] \n",
        "    \n",
        "  def call(self, x):\n",
        "    \n",
        "    for layer in self.layer_list:\n",
        "\n",
        "      x = layer(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  def reset_metrics(self):\n",
        "      \n",
        "    for metric in self.metrics:\n",
        "      metric.reset_states()\n",
        "          \n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "      \n",
        "      #print(tf.shape(data))\n",
        "      x, target = data\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(x)\n",
        "          \n",
        "          loss = self.loss_function(target, predictions) + tf.reduce_sum(self.losses)\n",
        "      \n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "      \n",
        "      # update loss metric\n",
        "      self.metrics[0].update_state(loss)\n",
        "      \n",
        "      # for all metrics except loss, update states (accuracy etc.)\n",
        "      for metric in self.metrics[1:]:\n",
        "          metric.update_state(target, predictions)\n",
        "      # Return a dictionary mapping metric names to current value\n",
        "      return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gyk3HWLBDExc"
      },
      "outputs": [],
      "source": [
        "# initialize models \n",
        "model = MyModel()\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Training for more than 300 epochs would be required for meaningful results \n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b9A_oL4BDCh8"
      },
      "outputs": [],
      "source": [
        "# Define saving location for log\n",
        "hyperparameter_string= f\"Trial:01_BATCH:{BATCH_SIZE}_EPOCH:{EPOCHS}_Adam\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_path = f\"logs/{hyperparameter_string}/{current_time}/train\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wFDh6qO5BmLv"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def training(model, train_ds, epochs):\n",
        "\n",
        "  model = model\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch}:\")\n",
        "\n",
        "    # Training:\n",
        "\n",
        "    for data in train_ds:\n",
        "        metrics = model.train_step(data)\n",
        "\n",
        "    # print the metrics\n",
        "    print([f\"{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "\n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with train_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics()\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH-Ubg18DHbx",
        "outputId": "bf9989b8-c065-45a5-8196-9e3d8d4da90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "['loss: 6.1695380210876465', 'acc: 0.004481531213968992']\n",
            "\n",
            "\n",
            "Epoch 1:\n",
            "['loss: 5.956798076629639', 'acc: 0.005129252560436726']\n",
            "\n",
            "\n",
            "Epoch 2:\n",
            "['loss: 5.274999618530273', 'acc: 0.003956351894885302']\n",
            "\n",
            "\n",
            "Epoch 3:\n",
            "['loss: 5.054451942443848', 'acc: 0.005006710533052683']\n",
            "\n",
            "\n",
            "Epoch 4:\n",
            "['loss: 4.917806625366211', 'acc: 0.004784968215972185']\n",
            "\n",
            "\n",
            "Epoch 5:\n",
            "['loss: 4.809929847717285', 'acc: 0.00373460934497416']\n",
            "\n",
            "\n",
            "Epoch 6:\n",
            "['loss: 4.7243242263793945', 'acc: 0.004096399527043104']\n",
            "\n",
            "\n",
            "Epoch 7:\n",
            "['loss: 4.648908615112305', 'acc: 0.002655073767527938']\n",
            "\n",
            "\n",
            "Epoch 8:\n",
            "['loss: 4.580646514892578', 'acc: 0.003676256164908409']\n",
            "\n",
            "\n",
            "Epoch 9:\n",
            "['loss: 4.516909599304199', 'acc: 0.0031977591570466757']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train the model \n",
        "training(model, text_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model with a meaningful name\n",
        "model.save_weights(f\"saved_model_{hyperparameter_string}\", save_format=\"tf\")"
      ],
      "metadata": {
        "id": "K59i1WOTiOhE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a new model \n",
        "loaded_model = MyModel()\n",
        "\n",
        "# load the model weights to continue training. \n",
        "loaded_model.load_weights(f\"saved_model_{hyperparameter_string}\");"
      ],
      "metadata": {
        "id": "udSv-O2AiWtF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dor1z9zbDGud"
      },
      "outputs": [],
      "source": [
        "def generate_text(input, top_k=100):\n",
        "\n",
        "  input = tokenizer.tokenize(input)\n",
        "  #print(input)\n",
        "\n",
        "  padding_size = m - len(input)\n",
        "\n",
        "  paddings = tf.constant([[0, 0,], [padding_size, 0]])\n",
        "\n",
        "  input =  tf.pad([input], paddings, \"CONSTANT\")\n",
        "  #print(input.shape)\n",
        "\n",
        "  output = model(input)\n",
        "\n",
        "  output1 = tokenizer.detokenize(tf.cast(output, dtype=tf.int32))\n",
        "  #print(output1)\n",
        "  #output = tf.expand_dims(, axis=0)\n",
        "  #print(input.shape)\n",
        "\n",
        "  logits, indices = tf.math.top_k(output, top_k)\n",
        "  #print(indices)\n",
        "  output = tf.random.categorical(logits,1)\n",
        "  #print(output.shape)\n",
        "  #print(output)\n",
        "\n",
        "  output = tf.cast(output, dtype=tf.int32)\n",
        "  output = tokenizer.detokenize(output)\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'What is the next word that should be'\n",
        "\n",
        "generate_text(text)"
      ],
      "metadata": {
        "id": "kaf1kSBpzyt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "sheet11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}